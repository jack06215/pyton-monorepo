{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training\n",
    "\n",
    "- 学習データの準備\n",
    "- 言語モデルとは\n",
    "- ニューラルネットワークを使用しない手法(uni-gram, bi-gram)\n",
    "- Transformer 以前のニューラルネットワークを使用した言語モデル\n",
    "  - MLP\n",
    "  - RNN\n",
    "- Transformer を使用した言語モデル\n",
    "  - Self-Attention と Feedforward Network の実装、並列化\n",
    "  - GPT-2 の実装\n",
    "\n",
    "プログラミングの解説・エラー解消や用語の解説 GPT  \n",
    "https://chatgpt.com/g/g-H1Baw636t-mlxian-bei\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データの準備\n",
    "\n",
    "ChatGPT のような大規模言語モデルの学習には文書データを大量に集める必要があります。  \n",
    "データの集め方によってモデルの性能が大きく左右されるので、学習データの準備は重要な工程です。以下に参考になりそうな論文を載せておきます。\n",
    "\n",
    "- [Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/abs/2402.00159)\n",
    "- [FineWeb: decanting the web for the finest text data at scale](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)\n",
    "- [Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance](https://arxiv.org/abs/2403.16952)\n",
    "- [To Code, or Not To Code? Exploring Impact of Code in Pre-training](https://arxiv.org/abs/2408.10914)\n",
    "- [Instruction Pre-Training: Language Models are Supervised Multitask Learners](https://arxiv.org/abs/2406.14491)\n",
    "- [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644)\n",
    "- [Physics of Language Models: Part 3.1, Knowledge Storage and Extraction](https://arxiv.org/abs/2309.14316)\n",
    "- [松尾・岩澤研究室で開催した LLM 勉強会での発表資料](https://docs.google.com/presentation/d/14SeP11PcgmNcl93Xt0ziSynxdrOVd2WM/edit?usp=sharing&ouid=101802221278095300433&rtpof=true&sd=true)\n",
    "\n",
    "今回は[LLM-jp という団体](https://llm-jp.nii.ac.jp/)が整備したコーパスを利用します。\n",
    "\n",
    "- https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "!wget --no-check-certificate https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2/-/raw/main/ja/ja_wiki/train_9.jsonl.gz\n",
    "!wget --no-check-certificate https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2/-/raw/main/ja/ja_wiki/validation_0.jsonl.gz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'meta'])\n",
      "チリコシーは、アメリカ合衆国オハイオ州中央部南ロス郡の都市であり、同郡の郡庁所在地である。コロンバス大都市圏に属している。\n",
      "\n",
      "2010年の国勢調査では人口21,901 人だった。ロス郡では唯一の都市で\n",
      "{'id': '2973866', 'title': 'チリコシー (オハイオ州)', 'url': 'https://ja.wikipedia.org/wiki/%E3%83%81%E3%83%AA%E3%82%B3%E3%82%B7%E3%83%BC%20%28%E3%82%AA%E3%83%8F%E3%82%A4%E3%82%AA%E5%B7%9E%29'}\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "mini_train_data_file_num = 1000\n",
    "mini_train_data_text = \"\"\n",
    "file_count = 0\n",
    "with gzip.open(\"./train_9.jsonl.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        mini_train_data_text += data[\"text\"] + \"\\n\"\n",
    "        file_count += 1\n",
    "        if file_count == mini_train_data_file_num:\n",
    "            break\n",
    "\n",
    "print(data.keys())\n",
    "print(data[\"text\"][:100])\n",
    "print(data[\"meta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'meta'])\n",
      "梶原 一騎（かじわら いっき、1936年9月4日 - 1987年1月21日）は、日本の漫画原作者、小説家、映画プロデューサー。本名は高森 朝樹（たかもり あさき）。高森 朝雄（たかもり あさお）の筆名\n",
      "{'id': '506', 'title': '梶原一騎', 'url': 'https://ja.wikipedia.org/wiki/%E6%A2%B6%E5%8E%9F%E4%B8%80%E9%A8%8E'}\n"
     ]
    }
   ],
   "source": [
    "val_data_file_num = 1\n",
    "val_data_text = \"\"\n",
    "file_count = 0\n",
    "with gzip.open(\"validation_0.jsonl.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        # 各行をJSONとして読み込む\n",
    "        data = json.loads(line)\n",
    "        val_data_text += data[\"text\"] + \"\\n\"\n",
    "        file_count += 1\n",
    "        if file_count == val_data_file_num:\n",
    "            break\n",
    "print(data.keys())\n",
    "print(data[\"text\"][:100])\n",
    "print(data[\"meta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 言語モデルとは\n",
    "\n",
    "大規模言語モデルの\"言語モデル\"とは、単語列の出現確率をモデル化したものです。  \n",
    "確率を計算できるので、与えられた文章がよく見る文章(確率が高い)なのか、変な文章なのか(確率が低い)を判断することができたり、新たに文章を生成(確率に従ってくじ引きをする)することができます。\n",
    "より良い言語モデルの開発のためには、データとどのように確率をモデル化するかのデザインが重要です。\n",
    "\n",
    "### データについて\n",
    "\n",
    "言語モデルはデータを元に学習するため、そのデータに有益な情報(例えば日本の歴史や法律に関する文章)が含まれていないと、単語自身の理解や単語同士の関係性を学習することができません。  \n",
    "人間が本を読んだり、人との会話を通じて新しい知識を得たり、良い文章の書き方を学んだりするように、言語モデルもデータを通じて学習します。\n",
    "\n",
    "### モデル化について\n",
    "\n",
    "データを得たとしても、どのように確率をモデル化するかのデザインがうまくいかないと、良い言語モデルは作れません。\n",
    "\n",
    "- データ中の文字を数え上げて前の 1 単語から次の 1 単語を予測するモデル\n",
    "- ニューラルネットワークの一種である MLP を用いて、前の数単語から次の 1 単語を予測するモデル\n",
    "- Transformer を用いて、より長い文脈を考慮して次の単語を予測するモデル\n",
    "\n",
    "を開発します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークを使用しない、数え上げによる手法(uni-gram, bi-gram)の言語モデル\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram 言語モデル\n",
    "\n",
    "$P\\left(w_1, \\ldots, w_m\\right)=\\prod_{i=1}^{i=m} P\\left(w_i \\mid w_1, \\ldots, w_{i-1}\\right) \\approx \\prod_{i=1}^{i=m} P\\left(w_i \\mid w_{i-n}, \\ldots, w_{i-1}\\right)$\n",
    "\n",
    "#### uni-gram モデル\n",
    "\n",
    "$P\\left(w_1, \\ldots, w_m\\right) \\approx \\prod_{i=1}^{i=m} P(w_i)$\n",
    "\n",
    "#### bi-gram モデル\n",
    "\n",
    "$P\\left(w_1, \\ldots, w_m\\right)\\approx \\prod_{i=1}^{i=m} P\\left(w_i \\mid w_{i-1}\\right)$\n",
    "\n",
    "$P\\left(w_{i} \\mid w_{i-1}\\right)=\\frac{\\operatorname{count}\\left(w_{i}, w_{i-1}\\right)}{\\operatorname{count}\\left(w_{i}\\right)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "『勝つか死ぬか』はHBO(日本ではスター・チャンネルが放送)のファンタジー・ドラマ・シリーズである『ゲーム・オブ・スローンズ』の第1章『七王国戦記』の第7話である。プロデューサーでもあるデイヴィッド・\n",
      "------------------------\n",
      "全体の文字数:  1569582\n",
      "文字の種類:  3807\n"
     ]
    }
   ],
   "source": [
    "print(mini_train_data_text[:100])\n",
    "print(\"------------------------\")\n",
    "print(\"全体の文字数: \", len(mini_train_data_text))\n",
    "print(\"文字の種類: \", len(set(mini_train_data_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出現頻度の高い文字\n",
      "[(' ', 53441), ('\\n', 44189), ('の', 38675), ('、', 30453), ('ー', 29588)]\n",
      "\n",
      "'日'の確率 = '日'の出現回数 ÷ 全体の文字数 =  7361 ÷ 1569582 = 0.004689783649404746\n",
      "'日本'の確率 = '日'の確率 x '本'の確率 = 0.004689783649404746 x 0.001984604818352912 = 9.307367227641361e-06\n",
      "'日曜'の確率 = '日'の確率 x '曜'の確率 = 0.004689783649404746 x 0.00018093989355127672 = 8.485689543018127e-07\n"
     ]
    }
   ],
   "source": [
    "character_count = {}\n",
    "for character in mini_train_data_text:\n",
    "    character_count[character] = character_count.get(character, 0) + 1\n",
    "# 出現頻度の高い文字を上位5個表示\n",
    "print(\"出現頻度の高い文字\")\n",
    "print(sorted(character_count.items(), key=lambda kv: -kv[1])[:5])\n",
    "\n",
    "# 過去の情報を考慮しないuni-gramモデル\n",
    "print()\n",
    "total_count = sum(character_count.values())\n",
    "ch_0 = \"日\"\n",
    "ch_1 = \"本\"\n",
    "ch_2 = \"曜\"\n",
    "print(\n",
    "    f\"'{ch_0}'の確率 = '{ch_0}'の出現回数 ÷ 全体の文字数 =  {character_count[ch_0]} ÷ {total_count} = {character_count[ch_0] / total_count}\"\n",
    ")\n",
    "print(\n",
    "    f\"'{ch_0 + ch_1}'の確率 = '{ch_0}'の確率 x '{ch_1}'の確率 = {character_count[ch_0] / total_count} x {character_count[ch_1] / total_count} = {character_count[ch_0] * character_count[ch_1] / total_count ** 2}\"\n",
    ")\n",
    "print(\n",
    "    f\"'{ch_0 + ch_2}'の確率 = '{ch_0}'の確率 x '{ch_2}'の確率 = {character_count[ch_0] / total_count} x {character_count[ch_2] / total_count} = {character_count[ch_0] * character_count[ch_2] / total_count ** 2}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 出現確率の可視化\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "total_count = sum(character_count.values())\n",
    "# 上位30文字の出現頻度を取る\n",
    "top_k = 30\n",
    "top_k_key = sorted(character_count.keys(), key=lambda k: -character_count[k])[:top_k]\n",
    "top_k_value = [character_count[k] for k in top_k_key]\n",
    "top_k_probability = [v / total_count for v in top_k_value]\n",
    "\n",
    "# x軸に文字、y軸に出現頻度を取る\n",
    "plt.bar(top_k_key, top_k_probability)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "' '\n",
      "'『勝つか                    '\n"
     ]
    }
   ],
   "source": [
    "# 生成\n",
    "# greedy decoding\n",
    "max_tokens = 20\n",
    "generated_text = \"『勝つか\"\n",
    "for _ in range(max_tokens):\n",
    "    next_char = max(\n",
    "        character_count,\n",
    "        key=lambda k: character_count[k],\n",
    "    )  # 最も出現頻度が高い文字を選択, greedy decoding\n",
    "    print(repr(next_char))\n",
    "    generated_text += next_char\n",
    "\n",
    "print(repr(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'決'\n",
      "'ア'\n",
      "' '\n",
      "'持'\n",
      "'J'\n",
      "'ん'\n",
      "'コ'\n",
      "'の'\n",
      "'部'\n",
      "'と'\n",
      "' '\n",
      "'ど'\n",
      "'が'\n",
      "'t'\n",
      "'く'\n",
      "'。'\n",
      "'に'\n",
      "'、'\n",
      "' '\n",
      "'り'\n",
      "'〜'\n",
      "'\\n'\n",
      "'」'\n",
      "'っ'\n",
      "'6'\n",
      "'の'\n",
      "'a'\n",
      "'2'\n",
      "'こ'\n",
      "'プ'\n",
      "'『勝つか決ア 持Jんコの部と どがtく。に、 り〜\\n」っ6のa2こプ'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "max_tokens = 30\n",
    "top_p = 0.9\n",
    "generated_text = \"『勝つか\"\n",
    "\n",
    "for _ in range(max_tokens):\n",
    "    sorted_character_count = sorted(character_count.items(), key=lambda kv: -kv[1])\n",
    "    cumulative_probability = 0.0\n",
    "    vocab = []\n",
    "    vocab_count = []\n",
    "    vocab_total_count = 0\n",
    "    for character, count in sorted_character_count:\n",
    "        cumulative_probability += count / total_count\n",
    "        if cumulative_probability > top_p:\n",
    "            break\n",
    "        vocab.append(character)\n",
    "        vocab_count.append(count)\n",
    "        vocab_total_count += count\n",
    "    probability = [count / vocab_total_count for count in vocab_count]\n",
    "    next_char = random.choices(vocab, probability)[0]\n",
    "    print(repr(next_char))\n",
    "    generated_text += next_char\n",
    "\n",
    "print(repr(generated_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Tokenizer の設計も重要です、以下に参考となるリンクを載せておきます。\n",
    "\n",
    "- [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)\n",
    "- [Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies](https://arxiv.org/abs/2407.13623)\n",
    "- [ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models](https://arxiv.org/abs/2105.13626)\n",
    "- GPT シリーズの Tokenizer: https://github.com/openai/tiktoken\n",
    "- Llama2 や LLM-jp の tokenizer を作成する際に利用: https://github.com/google/sentencepiece\n",
    "- Byte Pair Encoding の実装: https://github.com/kenoharada/language-model-from-scratch/blob/main/notebooks/Ja/Tokenizer.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3310, 3354, 694, 667, 703, 591, 1027, 1621, 592, 2156, 568, 549, 584, 570, 526]\n"
     ]
    }
   ],
   "source": [
    "unique_chars_in_train_text = sorted(list(set(mini_train_data_text)))\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, chars):\n",
    "        self.str_to_idx = dict()\n",
    "        self.str_to_idx[\"<|endoftext|>\"] = 0\n",
    "        # utf-8\n",
    "        for i in range(256):\n",
    "            if f\"<utf8_{i}>\" not in self.str_to_idx:\n",
    "                self.str_to_idx[f\"<utf8_{i}>\"] = len(self.str_to_idx)\n",
    "        for char in chars:\n",
    "            self.str_to_idx[char] = len(self.str_to_idx)\n",
    "        self.idx_to_str = dict()\n",
    "        for key, value in self.str_to_idx.items():\n",
    "            self.idx_to_str[value] = key\n",
    "\n",
    "    def encode(self, text, eot=False):\n",
    "        result = []\n",
    "        for char in text:\n",
    "            if char not in self.str_to_idx:\n",
    "                utf_8_num = list(char.encode(\"utf-8\"))\n",
    "                for num in utf_8_num:\n",
    "                    result.append(self.str_to_idx[f\"<utf8_{num}>\"])\n",
    "            else:\n",
    "                result.append(self.str_to_idx[char])\n",
    "        if eot:\n",
    "            result.append(self.str_to_idx[\"<|endoftext|>\"])\n",
    "        return result\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        decoded_with_utf_token = [self.idx_to_str[token] for token in tokens]\n",
    "        decoded_postprocess_utf = []\n",
    "        utf_tokens = []\n",
    "        for token in decoded_with_utf_token:\n",
    "            if token.startswith(\"<utf8_\"):\n",
    "                utf_num = int(token.replace(\"<utf8_\", \"\").replace(\">\", \"\"))\n",
    "                utf_tokens.append(utf_num)\n",
    "            else:\n",
    "                if utf_tokens:\n",
    "                    decoded_postprocess_utf.append(bytes(utf_tokens).decode(\"utf-8\"))\n",
    "                    utf_tokens = []\n",
    "                decoded_postprocess_utf.append(token)\n",
    "        if utf_tokens:\n",
    "            decoded_postprocess_utf.append(bytes(utf_tokens).decode(\"utf-8\"))\n",
    "            utf_tokens = []\n",
    "        return \"\".join(decoded_postprocess_utf)\n",
    "\n",
    "    def decode_with_utf(self, tokens):\n",
    "        return \"\".join([self.idx_to_str[token] for token in tokens])\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    unique_chars_in_train_text\n",
    ")  # Tokenizerの初期化、一般的にはByte Pair EncodingやUnigram Language Modelなどを活用してTokenizerを実装する\n",
    "text = \"言語モデルの勉強は楽しいです。\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習用データにある語\n",
    "\"日\" in unique_chars_in_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1954], '日', '日')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"日\"), tokenizer.decode_with_utf([1954]), tokenizer.decode([1954])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習用データにない未知語\n",
    "\"😄\" in unique_chars_in_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([241, 160, 153, 133], '<utf8_240><utf8_159><utf8_152><utf8_132>', '😄')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    tokenizer.encode(\"😄\"),\n",
    "    tokenizer.decode_with_utf([241, 160, 153, 133]),\n",
    "    tokenizer.decode([241, 160, 153, 133]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出現頻度の高いbigram\n",
      "[((258, 259), 14841), ((526, 258), 10469), ((258, 258), 9382), ((620, 526), 8329), ((277, 275), 7166), ((259, 272), 6726), ((272, 259), 6423), ((576, 526), 5829), ((583, 549), 5599), ((568, 583), 5552)]\n",
      "'\\n' ' ' 14841\n",
      "'。' '\\n' 10469\n",
      "'\\n' '\\n' 9382\n",
      "'る' '。' 8329\n",
      "'2' '0' 7166\n",
      "' ' '-' 6726\n",
      "'-' ' ' 6423\n",
      "'た' '。' 5829\n",
      "'て' 'い' 5599\n",
      "'し' 'て' 5552\n"
     ]
    }
   ],
   "source": [
    "# bi-gramモデル\n",
    "bigram_count = {}\n",
    "mini_train_data_tokens = []\n",
    "mini_train_data_file_num = 1000\n",
    "mini_train_data_text = \"\"\n",
    "file_count = 0\n",
    "\n",
    "with gzip.open(\"train_9.jsonl.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        # 各行をJSONとして読み込む\n",
    "        data = json.loads(line)\n",
    "        mini_train_data_tokens += tokenizer.encode(data[\"text\"], eot=True)\n",
    "        file_count += 1\n",
    "        if file_count == mini_train_data_file_num:\n",
    "            break\n",
    "\n",
    "val_data_tokens = []\n",
    "val_data_file_num = 1\n",
    "val_data_text = \"\"\n",
    "file_count = 0\n",
    "with gzip.open(\"validation_0.jsonl.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        # 各行をJSONとして読み込む\n",
    "        data = json.loads(line)\n",
    "        val_data_tokens += tokenizer.encode(data[\"text\"], eot=True)\n",
    "        file_count += 1\n",
    "        if file_count == val_data_file_num:\n",
    "            break\n",
    "\n",
    "for i in range(len(mini_train_data_tokens) - 1):\n",
    "    bigram = (mini_train_data_tokens[i], mini_train_data_tokens[i + 1])\n",
    "    bigram_count[bigram] = bigram_count.get(bigram, 0) + 1\n",
    "\n",
    "# top-kのbigramを表示\n",
    "print(\"出現頻度の高いbigram\")\n",
    "top_k = 10\n",
    "top_k_bigram = sorted(bigram_count.items(), key=lambda kv: -kv[1])[:top_k]\n",
    "print(top_k_bigram)\n",
    "# decodeして確認\n",
    "for bigram, count in top_k_bigram:\n",
    "    print(\n",
    "        repr(tokenizer.decode([bigram[0]])), repr(tokenizer.decode([bigram[1]])), count\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ら'\n",
      "'れ'\n",
      "'た'\n",
      "'。'\n",
      "'\\n'\n",
      "' '\n",
      "'-'\n",
      "' '\n",
      "'-'\n",
      "' '\n",
      "'-'\n",
      "' '\n",
      "'-'\n",
      "' '\n",
      "'-'\n",
      "' '\n",
      "'-'\n",
      "' '\n",
      "'-'\n",
      "' '\n",
      "[536, 1032, 581, 556, 618, 621, 576, 526, 258, 259, 272, 259, 272, 259, 272, 259, 272, 259, 272, 259, 272, 259, 272, 259]\n",
      "『勝つかられた。\n",
      " - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "# 生成\n",
    "# greedy decoding\n",
    "max_tokens = 20\n",
    "generated_text = \"『勝つか\"\n",
    "generated_tokens = tokenizer.encode(generated_text)\n",
    "for _ in range(max_tokens):\n",
    "    next_token = max(\n",
    "        bigram_count,\n",
    "        key=lambda k: bigram_count[k] if k[0] == generated_tokens[-1] else 0,\n",
    "    )  # 直前の文字が来たときの次の文字の出現頻度が最も高いものを選択\n",
    "    generated_tokens.append(next_token[1])\n",
    "    print(repr(tokenizer.decode([next_token[1]])))\n",
    "print(generated_tokens)\n",
    "print(tokenizer.decode(generated_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import copy\n",
    "\n",
    "\n",
    "class Ngram:\n",
    "    def __init__(self, n, vocab, laplace=1):\n",
    "        self.n = n\n",
    "        self.vocab = vocab\n",
    "        self.laplace = laplace\n",
    "        self.ngram = defaultdict(lambda: laplace)\n",
    "        self.context_count = defaultdict(lambda: laplace * len(self.vocab))\n",
    "\n",
    "    def train(self, token_list):\n",
    "        assert isinstance(token_list, list)\n",
    "        for i in range(len(token_list) - self.n + 1):\n",
    "            ngram_list = copy.deepcopy(token_list[i : i + self.n])\n",
    "            ngram_list = [str(i) for i in ngram_list]\n",
    "            context = ngram_list[:-1]\n",
    "            ngram_key = \"-\".join(ngram_list)\n",
    "            context_key = \"-\".join(context)\n",
    "            self.ngram[ngram_key] += 1\n",
    "            self.context_count[context_key] += 1\n",
    "\n",
    "    def train_batch(self, token_list):\n",
    "        for tokens in token_list:\n",
    "            self.train(tokens)\n",
    "\n",
    "    def get_prob(self, ngram):\n",
    "        if self.n == 1:\n",
    "            return self.ngram[ngram] / len(self.vocab)\n",
    "        else:\n",
    "            context = ngram.split(\"-\")[:-1]\n",
    "            context = \"-\".join(context)\n",
    "            return self.ngram[ngram] / self.context_count[context]\n",
    "\n",
    "    def get_prob_distribution(self, n_minus_1_gram):\n",
    "        distribution = []\n",
    "        distribution_dict = {}\n",
    "        for word in self.vocab:\n",
    "            ngram_list = n_minus_1_gram + [word]\n",
    "            ngram = \"-\".join([str(i) for i in ngram_list])\n",
    "            # print('hi', ngram)\n",
    "            distribution.append(self.get_prob(ngram))\n",
    "            distribution_dict[word] = self.get_prob(ngram)\n",
    "        return distribution, distribution_dict\n",
    "\n",
    "    def forward(self, token_indexes):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        if isinstance(token_indexes, torch.Tensor) or isinstance(\n",
    "            token_indexes, torch.LongTensor\n",
    "        ):\n",
    "            token_indexes = token_indexes.tolist()\n",
    "        assert isinstance(token_indexes, list)\n",
    "        batch_size = len(token_indexes)\n",
    "        sequence_length = len(token_indexes[0])\n",
    "        distributions = torch.ones(batch_size, sequence_length, len(self.vocab))\n",
    "        distributions /= len(self.vocab)\n",
    "        for i in range(sequence_length):\n",
    "            for batch in range(batch_size):\n",
    "                if self.n == 2:\n",
    "                    context = [token_indexes[batch][i]]\n",
    "                else:\n",
    "                    if i < self.n - 1:\n",
    "                        if i == 0:\n",
    "                            context = [token_indexes[batch][i]]\n",
    "                        else:\n",
    "                            context = token_indexes[batch][: i + 1]\n",
    "                    else:\n",
    "                        context = token_indexes[batch][i - self.n + 2 : i + 1]\n",
    "                distribution, _ = self.get_prob_distribution(context)\n",
    "                distributions[batch, i] = torch.tensor(distribution)\n",
    "        # distributions: (batch_size, sequence_length, vocab_size)\n",
    "        return distributions\n",
    "\n",
    "    def loss(self, token_indexes, targets):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        distributions = self.forward(token_indexes)\n",
    "        distributions = distributions.to(targets.device)\n",
    "        log_distributions = torch.log(distributions)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = log_distributions.shape\n",
    "        loss = F.nll_loss(\n",
    "            log_distributions.view(batch_size * sequence_length, vocab_size),\n",
    "            targets.view(batch_size * sequence_length),\n",
    "        )\n",
    "        # loss: scalar\n",
    "        return loss\n",
    "\n",
    "    def generate(self, token_indexes, max_length=10):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        for _ in range(max_length):\n",
    "            distributions = self.forward(token_indexes)\n",
    "            distributions = distributions[0, -1]\n",
    "            # greedy decoding\n",
    "            next_token = torch.argmax(distributions).item()\n",
    "            token_indexes[0].append(next_token)\n",
    "        return token_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パラメータ数: 16516096\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "parameters = len(tokenizer.str_to_idx) ** n\n",
    "print(f\"パラメータ数: {parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'『'\n",
      "'勝'\n",
      "'つ'\n",
      "'か'\n",
      "'ら'\n",
      "'れ'\n",
      "'た'\n",
      "'。'\n",
      "'\\n'\n",
      "' '\n",
      "'-'\n",
      "' '\n",
      "'-'\n",
      "' '\n",
      "'-'\n",
      "' '\n",
      "'-'\n",
      "' '\n",
      "'-'\n",
      "' '\n",
      "'-'\n",
      "' '\n",
      "'-'\n",
      "' '\n"
     ]
    }
   ],
   "source": [
    "ngram = Ngram(2, tokenizer.str_to_idx.values())\n",
    "ngram.train(mini_train_data_tokens)\n",
    "generated_text = \"『勝つか\"\n",
    "context_token_indexes = [tokenizer.encode(generated_text)]\n",
    "generated_tokens = ngram.generate(context_token_indexes, max_length=20)\n",
    "for token in generated_tokens[0]:\n",
    "    print(repr(tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.082467079162598"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 512\n",
    "input_tokens = val_data_tokens[:context_length]\n",
    "target_tokens = val_data_tokens[1 : context_length + 1]\n",
    "ngram.loss([input_tokens], torch.tensor([target_tokens])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'『'\n",
      "'勝'\n",
      "'つ'\n",
      "'か'\n",
      "'死'\n",
      "'ぬ'\n",
      "'か'\n",
      "'』'\n",
      "'は'\n",
      "'H'\n",
      "'B'\n",
      "'O'\n",
      "'('\n",
      "'日'\n",
      "'本'\n",
      "'で'\n",
      "'は'\n",
      "'ス'\n",
      "'タ'\n",
      "'ー'\n",
      "'・'\n",
      "'ウ'\n",
      "'ォ'\n",
      "'ー'\n"
     ]
    }
   ],
   "source": [
    "ngram = Ngram(4, tokenizer.str_to_idx.values())\n",
    "ngram.train(mini_train_data_tokens)\n",
    "generated_text = \"『勝つか\"\n",
    "context_token_indexes = [tokenizer.encode(generated_text)]\n",
    "generated_tokens = ngram.generate(context_token_indexes, max_length=20)\n",
    "for token in generated_tokens[0]:\n",
    "    print(repr(tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.280468463897705"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 512\n",
    "input_tokens = val_data_tokens[:context_length]\n",
    "target_tokens = val_data_tokens[1 : context_length + 1]\n",
    "ngram.loss([input_tokens], torch.tensor([target_tokens])).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークを使用した言語モデル\n",
    "\n",
    "言語モデルをデザインする際に、過去の文脈を考慮するとより自然な文章のモデリングができると考えられます。  \n",
    "ただ、N-gram モデルのパラメータ数のオーダーは、$O\\left(\\left|V\\right|^n\\right)$　となり、過去の文脈が増えれば増えるほど組み合わせが膨大になります。  \n",
    "組み合わせ単位で数え上げているため、データ中に出現しない組み合わせがあると、その組み合わせの確率は 0 となり、その後の予測ができなくなってしまいます。\n",
    "\n",
    "また、組み合わせ別の数え上げでは、単語・文脈をそれぞれ独立したものとして扱っており、単語の意味や文脈を共有した表現が得られません。\n",
    "\n",
    "解決策として組み合わせの表によるモデル化ではなく、ニューラルネットワークと単語ベクトルの表現を用いることでモデル化を行います。\n",
    "\n",
    "ニューラルネットワークの学習の際に行う、次単語予測によって単語ベクトルには単語の意味や概念が付与され、ニューラルネットワークのパラメータには単語同士の関係性が学習されることが期待されます。\n",
    "\n",
    "日本で最も高い山は「？」の？を当てるために、\n",
    "\n",
    "- ？には文法的に名詞が入る\n",
    "- その候補は文脈的に山であり\n",
    "- 日本で最も高いという情報がある、というような文脈上のどの情報に着目するか\n",
    "- 文脈を踏まえた上で今まで得た知識をどのように組み合わせるか\n",
    "\n",
    "ということをニューラルネットワークが学習します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP によるモデル化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        # 単語ベクトルの取得\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        # FeedForward Layer\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        # vocab_sizeへの変換\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        print(\"number of parameters:\", sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "    def forward(self, token_indexes):\n",
    "        # token_index: (batch_size, sequence_length)\n",
    "        embedding = self.token_embedding_table(token_indexes)\n",
    "        logits = self.linear(self.ff(embedding))\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def loss_per_token(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size * sequence_length, vocab_size),\n",
    "            targets.view(batch_size * sequence_length),\n",
    "            reduction=\"none\",\n",
    "        )\n",
    "        # loss: (batch_size*sequence_length)\n",
    "        return loss.view(batch_size, sequence_length)\n",
    "\n",
    "    def loss(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size * sequence_length, vocab_size),\n",
    "            targets.view(batch_size * sequence_length),\n",
    "        )\n",
    "        # loss: scalar\n",
    "        return loss\n",
    "\n",
    "    def generate(self, token_indexes, max_new_tokens):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length = token_indexes.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(token_indexes)\n",
    "            # logits: (batch_size, sequence_length, vocab_size)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            # next_token_logits: (batch_size, vocab_size)\n",
    "            next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "            # next_token_probs: (batch_size, vocab_size)\n",
    "            # greedy decoding\n",
    "            next_token = torch.argmax(next_token_probs, dim=-1, keepdim=True)\n",
    "            # next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "            # next_token: (batch_size, 1)\n",
    "            token_indexes = torch.cat([token_indexes, next_token], dim=1)\n",
    "            # token_indexes: (batch_size, sequence_length+1)\n",
    "        return token_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 36724\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.str_to_idx)\n",
    "d_model = 4\n",
    "d_ff = d_model * 4\n",
    "nn_lm = BigramLanguageModel(vocab_size, d_model, d_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 536, 1032,  581]])\n",
      "『勝つ\n",
      "tensor([[1032,  581,  556]])\n",
      "勝つか\n"
     ]
    }
   ],
   "source": [
    "text = \"『勝つか\"\n",
    "token_indexes = torch.tensor([tokenizer.encode(text)])\n",
    "input_token_indexes = token_indexes[:, :-1]\n",
    "target_token_indexes = token_indexes[:, 1:]\n",
    "print(input_token_indexes)\n",
    "print(tokenizer.decode(input_token_indexes[0].tolist()))\n",
    "\n",
    "print(target_token_indexes)\n",
    "print(tokenizer.decode(target_token_indexes[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 536, 1032,  581])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_indexes[0][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8765,  0.7550, -1.0909,  1.0455],\n",
      "         [-1.1829, -0.9189, -0.8844, -0.4369],\n",
      "         [ 0.3281, -0.2773, -1.3501,  2.0815]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = nn_lm.token_embedding_table(input_token_indexes)\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力: 『 → Token id: 536 → Token embedding: [-0.8764523863792419, 0.7549781203269958, -1.0909125804901123, 1.045540690422058]\n",
      "→ Neural Net →\n",
      "1032: 0.000, 2877: 0.001, 2715: 0.001, 1150: 0.000 <---学習によって近づける---> Target: 1032: 1, 2877: 0, 2715: 0, 1150: 0\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"入力: {input_text} → Token id: {input_token_id} → Token embedding: {token_embeddings}\n",
    "→ Neural Net →\n",
    "{next_token_prediction_prob} <---学習によって近づける---> Target: {target_prob}\"\"\"\n",
    "for idx, token in enumerate(input_token_indexes[0]):\n",
    "    input_text = tokenizer.decode([input_token_indexes[0].tolist()[idx]])\n",
    "    input_token_id = token.item()\n",
    "    token_embeddings = (\n",
    "        nn_lm.token_embedding_table(token.unsqueeze(0))\n",
    "        .squeeze()\n",
    "        .detach()\n",
    "        .numpy()\n",
    "        .tolist()\n",
    "    )\n",
    "    next_token_prediction = nn_lm(token.unsqueeze(0))\n",
    "    next_token_prediction = F.softmax(next_token_prediction, dim=-1)\n",
    "    next_token_prediction_top_k_index = torch.topk(next_token_prediction, 3).indices\n",
    "\n",
    "    target_token_id = target_token_indexes[0][idx].item()\n",
    "    pickup_token_indexes = [target_token_id] + next_token_prediction_top_k_index[\n",
    "        0\n",
    "    ].tolist()\n",
    "    next_token_prediction_prob = next_token_prediction[0][pickup_token_indexes].tolist()\n",
    "    next_token_prediction_prob_text = [\n",
    "        f\"{token_id}: {prob:.3f}\"\n",
    "        for token_id, prob in zip(pickup_token_indexes, next_token_prediction_prob)\n",
    "    ]\n",
    "    next_token_prediction_prob = \", \".join(next_token_prediction_prob_text)\n",
    "    target_token_distribution = F.one_hot(\n",
    "        target_token_indexes[0][idx], num_classes=vocab_size\n",
    "    )[pickup_token_indexes].tolist()\n",
    "    target_token_distribution = [\n",
    "        f\"{token_id}: {prob}\"\n",
    "        for token_id, prob in zip(pickup_token_indexes, target_token_distribution)\n",
    "    ]\n",
    "    target_token_distribution = \", \".join(target_token_distribution)\n",
    "    print(\n",
    "        template.format(\n",
    "            input_text=input_text,\n",
    "            input_token_id=input_token_id,\n",
    "            token_embeddings=token_embeddings,\n",
    "            next_token_prediction_prob=next_token_prediction_prob,\n",
    "            target_prob=target_token_distribution,\n",
    "        )\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 36724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training\n",
      "val_loss: 8.401255655669221\n",
      "start training\n",
      "epoch: 0, loss: 5.983282566070557, training_tokens: 320000\n",
      "epoch: 0, loss: 5.810821056365967, training_tokens: 640000\n",
      "epoch: 0, loss: 6.078439235687256, training_tokens: 960000\n",
      "epoch: 0, loss: 5.658401966094971, training_tokens: 1280000\n",
      "epoch: 0, val_loss: 6.141931521685497\n"
     ]
    }
   ],
   "source": [
    "# 学習\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nn_lm = BigramLanguageModel(vocab_size, d_model, d_ff).to(device)\n",
    "optimizer = torch.optim.AdamW(nn_lm.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 1\n",
    "training_tokens = 0\n",
    "print(\"before training\")\n",
    "val_loss = 0\n",
    "for i in range(0, len(val_data_tokens), batch_size):\n",
    "    batch_tokens = val_data_tokens[i : i + batch_size]\n",
    "    input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "    target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "    with torch.no_grad():\n",
    "        loss = nn_lm.loss_per_token(input_token_indexes, target_token_indexes)\n",
    "    val_loss += loss.sum().item()\n",
    "val_loss = val_loss / len(val_data_tokens)\n",
    "print(f\"val_loss: {val_loss}\")\n",
    "print(\"start training\")\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(mini_train_data_tokens), batch_size):\n",
    "        batch_tokens = mini_train_data_tokens[i : i + batch_size]\n",
    "        input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "        target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "        loss = nn_lm.loss(input_token_indexes, target_token_indexes)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_tokens += len(batch_tokens)\n",
    "        if training_tokens % 10000 == 0:\n",
    "            print(\n",
    "                f\"epoch: {epoch}, loss: {loss.item()}, training_tokens: {training_tokens}\"\n",
    "            )\n",
    "    val_loss = 0\n",
    "    for i in range(0, len(val_data_tokens), batch_size):\n",
    "        batch_tokens = val_data_tokens[i : i + batch_size]\n",
    "        input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "        target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "        with torch.no_grad():\n",
    "            loss = nn_lm.loss_per_token(input_token_indexes, target_token_indexes)\n",
    "        val_loss += loss.sum().item()\n",
    "    val_loss = val_loss / len(val_data_tokens)\n",
    "    print(f\"epoch: {epoch}, val_loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'『'\n",
      "'勝'\n",
      "'つ'\n",
      "'か'\n",
      "'1'\n",
      "' '\n",
      "'、'\n",
      "'の'\n",
      "' '\n",
      "'、'\n",
      "'の'\n",
      "' '\n",
      "'、'\n",
      "'の'\n",
      "' '\n",
      "'、'\n",
      "'の'\n",
      "' '\n",
      "'、'\n",
      "'の'\n",
      "' '\n",
      "'、'\n",
      "'の'\n",
      "' '\n"
     ]
    }
   ],
   "source": [
    "context = \"『勝つか\"\n",
    "context_token_indexes = torch.tensor(tokenizer.encode(context)).unsqueeze(0).to(device)\n",
    "generated_tokens = nn_lm.generate(context_token_indexes, max_new_tokens=20)\n",
    "for token in generated_tokens[0]:\n",
    "    print(repr(tokenizer.decode([token.item()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.047945022583008"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 512\n",
    "input_tokens = val_data_tokens[:context_length]\n",
    "target_tokens = val_data_tokens[1 : context_length + 1]\n",
    "input_token_indexes = torch.tensor(input_tokens).unsqueeze(0).to(device)\n",
    "target_token_indexes = torch.tensor(target_tokens).unsqueeze(0).to(device)\n",
    "nn_lm.loss(input_token_indexes, target_token_indexes).item()\n",
    "\n",
    "# bi-gram(パラメータ数 16516096)の性能: 5.082467079162598"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 4064\n",
      "8.309922989258318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(423.2439967775941, 7.38905609893065)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# 当てずっぽうモデルのloss\n",
    "vocab_size = len(tokenizer.str_to_idx)\n",
    "print(\"vocab_size\", vocab_size)\n",
    "print(-math.log(1 / len(tokenizer.str_to_idx.values())))\n",
    "\n",
    "(\n",
    "    math.exp(6.047948837280273),\n",
    "    math.exp(2.0),\n",
    ")  # Understanding Emergent Abilities of Language Models from the Loss Perspective, https://arxiv.org/abs/2403.15796"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演習 1: MLP の層を増やしたり、学習率などを変更して未来の単語の予測精度向上を試みる、過去の N トークンを入力とするモデルを構築してみよう\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "\n",
    "class NGramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_ff, num_layers, N):\n",
    "        super().__init__()\n",
    "        self.N = N  # 過去Nトークンを入力にする\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # より深いMLP層を構築する\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(d_model, d_ff))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(d_ff, d_model))\n",
    "        self.ff = nn.Sequential(*layers)\n",
    "\n",
    "        # vocab_sizeへの変換（最終的な出力）\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        print(\"number of parameters:\", sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "    def forward(self, token_indexes):\n",
    "        batch_size, sequence_length = token_indexes.shape\n",
    "\n",
    "        if sequence_length < self.N:\n",
    "            raise ValueError(f\"Input sequence length must be at least {self.N}\")\n",
    "\n",
    "        # 過去Nトークンだけを選択する\n",
    "        token_indexes = token_indexes[:, -self.N :]\n",
    "\n",
    "        # トークンを埋め込みベクトルに変換\n",
    "        embedding = self.token_embedding_table(token_indexes)\n",
    "\n",
    "        # 平均することで過去Nトークンの情報を集約\n",
    "        embedding_mean = embedding.mean(dim=1)  # (batch_size, d_model)\n",
    "\n",
    "        # FeedForward Networkに通す\n",
    "        logits = self.linear(self.ff(embedding_mean))\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def loss(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, vocab_size)\n",
    "        # targets: (batch_size) - シーケンスの最後のトークン\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def generate(self, token_indexes, max_new_tokens):\n",
    "        batch_size, sequence_length = token_indexes.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(token_indexes)\n",
    "            next_token_probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.argmax(next_token_probs, dim=-1, keepdim=True)\n",
    "            token_indexes = torch.cat([token_indexes, next_token], dim=1)\n",
    "        return token_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 25162720\n",
      "before training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 6.141931521685497\n",
      "start training\n",
      "epoch: 0, loss: 5.753693580627441, training_tokens: 320000\n",
      "epoch: 0, loss: 5.701624393463135, training_tokens: 640000\n",
      "epoch: 0, loss: 5.988581657409668, training_tokens: 960000\n",
      "epoch: 0, loss: 5.535942554473877, training_tokens: 1280000\n",
      "epoch: 0, val_loss: 6.05252379290157\n",
      "epoch: 1, val_loss: 6.028843944287303\n"
     ]
    }
   ],
   "source": [
    "d_model = 512  # 埋め込み次元\n",
    "d_ff = 2048  # フィードフォワード層の次元\n",
    "num_layers = 10  # MLPの層数\n",
    "N = 4  # 過去Nトークンを入力とする\n",
    "learning_rate = 1e-4  # 学習率\n",
    "\n",
    "# 学習\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NGramLanguageModel(vocab_size, d_model, d_ff, num_layers, N).to(device)\n",
    "optimizer = torch.optim.AdamW(nn_lm.parameters(), lr=learning_rate)\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 2\n",
    "training_tokens = 0\n",
    "print(\"before training\")\n",
    "val_loss = 0\n",
    "for i in range(0, len(val_data_tokens), batch_size):\n",
    "    batch_tokens = val_data_tokens[i : i + batch_size]\n",
    "    input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "    target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "    with torch.no_grad():\n",
    "        loss = nn_lm.loss_per_token(input_token_indexes, target_token_indexes)\n",
    "    val_loss += loss.sum().item()\n",
    "val_loss = val_loss / len(val_data_tokens)\n",
    "print(f\"val_loss: {val_loss}\")\n",
    "print(\"start training\")\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(mini_train_data_tokens), batch_size):\n",
    "        batch_tokens = mini_train_data_tokens[i : i + batch_size]\n",
    "        input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "        target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "        loss = nn_lm.loss(input_token_indexes, target_token_indexes)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_tokens += len(batch_tokens)\n",
    "        if training_tokens % 10000 == 0:\n",
    "            print(\n",
    "                f\"epoch: {epoch}, loss: {loss.item()}, training_tokens: {training_tokens}\"\n",
    "            )\n",
    "    val_loss = 0\n",
    "    for i in range(0, len(val_data_tokens), batch_size):\n",
    "        batch_tokens = val_data_tokens[i : i + batch_size]\n",
    "        input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "        target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "        with torch.no_grad():\n",
    "            loss = nn_lm.loss_per_token(input_token_indexes, target_token_indexes)\n",
    "        val_loss += loss.sum().item()\n",
    "    val_loss = val_loss / len(val_data_tokens)\n",
    "    print(f\"epoch: {epoch}, val_loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 536, 1032,  581,  556,  620,  259,  525,  591,  591,  591,  591,  591,\n",
      "          591,  591,  591,  591,  591,  591,  591,  591,  591,  591,  591,  591]])\n",
      "'『'\n",
      "'勝'\n",
      "'つ'\n",
      "'か'\n",
      "'る'\n",
      "' '\n",
      "'、'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n",
      "'の'\n"
     ]
    }
   ],
   "source": [
    "context = \"『勝つか\"\n",
    "context_token_indexes = torch.tensor(tokenizer.encode(context)).unsqueeze(0).to(device)\n",
    "generated_tokens = nn_lm.generate(context_token_indexes, max_new_tokens=20)\n",
    "print(generated_tokens)\n",
    "for token in generated_tokens[0]:\n",
    "    print(repr(tokenizer.decode([token.item()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "言語ベクトルと MLP によるモデル化によって\n",
    "\n",
    "- 密な言語ベクトル表現によって、単語の意味や概念を表現\n",
    "- パラメータ数が O(exp(n))から O(n)へと削減\n",
    "\n",
    "残る課題\n",
    "\n",
    "- 見れる過去の文脈長が固定、増やそうとするとパラメータ数が増加\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN によるモデル化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model input [1, 3, 3, 1] ['い', 'え', 'え', 'い']\n",
      "model target [3, 3, 1, 5] ['え', 'え', 'い', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vocab = {\"あ\": 0, \"い\": 1, \"う\": 2, \"え\": 3, \"お\": 4, \"<|endoftext|>\": 5}\n",
    "idx_to_ch = dict((v, k) for (k, v) in vocab.items())\n",
    "text = \"いええい\"\n",
    "text = list(text) + [\"<|endoftext|>\"]\n",
    "text = [vocab[ch] for ch in text]\n",
    "model_input = torch.tensor(text[:-1])\n",
    "model_target = torch.tensor(text[1:])\n",
    "print(\n",
    "    \"model input\",\n",
    "    model_input.tolist(),\n",
    "    [idx_to_ch[idx] for idx in model_input.tolist()],\n",
    ")\n",
    "print(\n",
    "    \"model target\",\n",
    "    model_target.tolist(),\n",
    "    [idx_to_ch[idx] for idx in model_target.tolist()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNの定義\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 5\n",
    "hidden_dim = 3\n",
    "hidden_start = torch.zeros((1, hidden_dim)).T\n",
    "word_embedding_table = torch.randn((vocab_size, embedding_dim))\n",
    "\n",
    "We = torch.randn((hidden_dim, embedding_dim))\n",
    "Wh = torch.randn((hidden_dim, hidden_dim))\n",
    "Wy = torch.randn((vocab_size, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['い'] ----> RNN ---->  え\n",
      "P(え | い) = 0.292\n",
      "[0.027, 0.431, 0.026, 0.292, 0.129, 0.095] <-----学習によって近づける-----> [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "\n",
      "['い', 'え'] ----> RNN ---->  え\n",
      "P(え | い, え) = 0.190\n",
      "[0.063, 0.342, 0.182, 0.19, 0.2, 0.024] <-----学習によって近づける-----> [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "\n",
      "['い', 'え', 'え'] ----> RNN ---->  い\n",
      "P(い | い, え, え) = 0.226\n",
      "[0.345, 0.226, 0.198, 0.091, 0.09, 0.05] <-----学習によって近づける-----> [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "['い', 'え', 'え', 'い'] ----> RNN ---->  <|endoftext|>\n",
      "P(<|endoftext|> | い, え, え, い) = 0.118\n",
      "[0.166, 0.417, 0.049, 0.169, 0.081, 0.118] <-----学習によって近づける-----> [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RNNの順伝播の計算の様子\n",
    "h_t_minus_1 = hidden_start\n",
    "input_history = []\n",
    "\n",
    "# 前のステップの計算が次のステップの計算に影響するため並列化が難しい。\n",
    "# RNNの計算複雑度 len(model_input.tolist()) * hidden_dim * hidden_dim = T * d * d\n",
    "for t in range(len(model_input.tolist())):\n",
    "    idx = model_input.tolist()[t]\n",
    "    input_history.append(idx_to_ch[idx])\n",
    "    print(input_history, \"----> RNN ----> \", idx_to_ch[model_target.tolist()[t]])\n",
    "    x = torch.LongTensor([idx])\n",
    "    x = word_embedding_table[x].T\n",
    "    # Attentionを導入したいポイント、過去の文脈がh_t_minus_1に押し込まれる\n",
    "    # ネットワークが単語方向に深くなるため学習が不安定に\n",
    "    # d * d, 系列長によらず一定\n",
    "    h_t = torch.tanh(torch.matmul(We, x) + torch.matmul(Wh, h_t_minus_1))\n",
    "    logits = torch.matmul(Wy, h_t)\n",
    "    print(\n",
    "        f'P({idx_to_ch[model_target.tolist()[t]]} | {\", \".join(input_history)}) = {torch.softmax(logits, dim=0).squeeze().tolist()[model_target.tolist()[t]]:.3f}'\n",
    "    )\n",
    "    model_target_onehot = torch.zeros((1, vocab_size))\n",
    "    model_target_onehot[0, model_target.tolist()[t]] = 1\n",
    "    h_t_minus_1 = h_t\n",
    "    output_dist = [\n",
    "        float(\"{:.3f}\".format(output))\n",
    "        for output in torch.softmax(logits, dim=0).squeeze().tolist()\n",
    "    ]\n",
    "    print(\n",
    "        f\"{output_dist} <-----学習によって近づける-----> {model_target_onehot.tolist()[0]}\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN によるモデル化によって\n",
    "\n",
    "- 文脈長を固定せずに、任意の長さの文脈を考慮\n",
    "- 文脈長が増えてもパラメータ数が増えない\n",
    "\n",
    "残る課題\n",
    "\n",
    "- 学習が不安定(勾配消失、勾配爆発)\n",
    "- 並列化ができず、学習が遅い\n",
    "- 文脈長が長くなるとトークンの長距離依存関係の把握が難しくなる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演習 2: RNN で学習、生成してみよう\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # トークンの埋め込み層\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # RNN層 (ここではLSTMを使用)\n",
    "        self.rnn = nn.LSTM(d_model, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # 出力をvocab_size次元に変換する全結合層\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, token_indexes, hidden_state=None):\n",
    "        embedding = self.embedding(token_indexes)\n",
    "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, hidden_state\n",
    "\n",
    "    def loss(self, token_indexes, targets, hidden_state=None):\n",
    "        logits, hidden_state = self(token_indexes, hidden_state)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size * sequence_length, vocab_size),\n",
    "            targets.view(batch_size * sequence_length)\n",
    "        )\n",
    "        return loss, hidden_state\n",
    "\n",
    "    def loss_per_token(self, token_indexes, targets, hidden_state=None):\n",
    "        logits, hidden_state = self(token_indexes, hidden_state)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size * sequence_length, vocab_size),\n",
    "            targets.view(batch_size * sequence_length),\n",
    "            reduction='none'\n",
    "        )\n",
    "        return loss.view(batch_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "vocab_size = 10000  # 語彙のサイズ\n",
    "d_model = 512  # 埋め込み次元\n",
    "hidden_size = 2048  # RNNの隠れ層の次元\n",
    "num_layers = 4  # RNNのレイヤー数\n",
    "learning_rate = 1e-4  # 学習率\n",
    "\n",
    "# GPUの設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# モデル初期化\n",
    "model = RNNLanguageModel(vocab_size, d_model, hidden_size, num_layers).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# バッチサイズとエポック\n",
    "batch_size = 512\n",
    "epochs = 2\n",
    "\n",
    "# トレーニングデータとバリデーションデータ (例)\n",
    "# mini_train_data_tokens と val_data_tokens は事前に用意されたトークンのシーケンス\n",
    "training_tokens = 0\n",
    "\n",
    "# # 事前のバリデーション損失計算\n",
    "# print(\"before training\")\n",
    "# val_loss = 0\n",
    "# for i in range(0, len(val_data_tokens), batch_size):\n",
    "#     batch_tokens = val_data_tokens[i : i + batch_size]\n",
    "#     input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "#     target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "#     with torch.no_grad():\n",
    "#         loss = model.loss_per_token(input_token_indexes, target_token_indexes)\n",
    "#     val_loss += loss.sum().item()\n",
    "# val_loss = val_loss / len(val_data_tokens)\n",
    "# print(f\"val_loss: {val_loss}\")\n",
    "\n",
    "# トレーニング開始\n",
    "print(\"start training\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # モデルをトレーニングモードに設定\n",
    "    for i in range(0, len(mini_train_data_tokens), batch_size):\n",
    "        batch_tokens = mini_train_data_tokens[i : i + batch_size]\n",
    "        input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "        target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "\n",
    "        # 損失計算\n",
    "        loss, _ = model.loss(input_token_indexes, target_token_indexes)\n",
    "\n",
    "        # 勾配をゼロにリセットしてからバックプロパゲーション\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_tokens += len(batch_tokens)\n",
    "\n",
    "        # 定期的に損失を出力\n",
    "        if training_tokens % 10000 == 0:\n",
    "            print(f\"epoch: {epoch}, loss: {loss.item()}, training_tokens: {training_tokens}\")\n",
    "\n",
    "    # エポックごとにバリデーション損失を計算\n",
    "    val_loss = 0\n",
    "    model.eval()  # モデルを評価モードに設定\n",
    "    for i in range(0, len(val_data_tokens), batch_size):\n",
    "        batch_tokens = val_data_tokens[i : i + batch_size]\n",
    "        input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "        target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "        with torch.no_grad():\n",
    "            loss = model.loss_per_token(input_token_indexes, target_token_indexes)\n",
    "        val_loss += loss.sum().item()\n",
    "    val_loss = val_loss / len(val_data_tokens)\n",
    "    print(f\"epoch: {epoch}, val_loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Tokens: tensor([5, 3, 2, 3, 4, 0, 4, 1, 0, 5, 4, 1, 0, 4, 5, 0, 3, 5, 4, 5, 4, 5, 5, 0,\n",
      "        4, 4, 3, 5, 5, 4, 3, 3, 5, 5, 3, 3, 1, 5, 0, 1, 3, 1, 1, 3, 1, 0, 0, 4,\n",
      "        4, 3, 3])\n",
      "'\\x04'\n",
      "'\\x02'\n",
      "'\\x01'\n",
      "'\\x02'\n",
      "'\\x03'\n",
      "'<|endoftext|>'\n",
      "'\\x03'\n",
      "'\\x00'\n",
      "'<|endoftext|>'\n",
      "'\\x04'\n",
      "'\\x03'\n",
      "'\\x00'\n",
      "'<|endoftext|>'\n",
      "'\\x03'\n",
      "'\\x04'\n",
      "'<|endoftext|>'\n",
      "'\\x02'\n",
      "'\\x04'\n",
      "'\\x03'\n",
      "'\\x04'\n",
      "'\\x03'\n",
      "'\\x04'\n",
      "'\\x04'\n",
      "'<|endoftext|>'\n",
      "'\\x03'\n",
      "'\\x03'\n",
      "'\\x02'\n",
      "'\\x04'\n",
      "'\\x04'\n",
      "'\\x03'\n",
      "'\\x02'\n",
      "'\\x02'\n",
      "'\\x04'\n",
      "'\\x04'\n",
      "'\\x02'\n",
      "'\\x02'\n",
      "'\\x00'\n",
      "'\\x04'\n",
      "'<|endoftext|>'\n",
      "'\\x00'\n",
      "'\\x02'\n",
      "'\\x00'\n",
      "'\\x00'\n",
      "'\\x02'\n",
      "'\\x00'\n",
      "'<|endoftext|>'\n",
      "'<|endoftext|>'\n",
      "'\\x03'\n",
      "'\\x03'\n",
      "'\\x02'\n",
      "'\\x02'\n"
     ]
    }
   ],
   "source": [
    "# トークンのシーケンスの初期状態（シード）\n",
    "seed_token_indexes = torch.randint(0, vocab_size, (batch_size, 1))  # 1トークンだけを初期状態として使用\n",
    "\n",
    "# 新しいトークンを生成\n",
    "model.eval()\n",
    "generated_tokens = model.generate(seed_token_indexes, max_new_tokens=50, temperature=1.0)\n",
    "\n",
    "# 結果の確認\n",
    "print(\"Generated Tokens:\", generated_tokens[0])\n",
    "for token in generated_tokens[0]:\n",
    "    print(repr(tokenizer.decode([token.item()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformerによるモデル化と学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention $(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$\n",
    "\n",
    "Attention Is All You Needの式(1)より"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "sequence_length = 4\n",
    "d_head = 2\n",
    "\n",
    "K = torch.randn(batch_size, sequence_length, d_head)\n",
    "Q = torch.randn(batch_size, sequence_length, d_head)\n",
    "V = torch.randn(batch_size, sequence_length, d_head)\n",
    "\n",
    "qk_dot_product = Q @ K.transpose(-2, -1)\n",
    "scaled_qk_dot_product = qk_dot_product / (d_head ** 0.5)\n",
    "\n",
    "attention_scores = torch.softmax(scaled_qk_dot_product, dim=-1)\n",
    "attention_output = attention_scores @ V\n",
    "\n",
    "attention_scores_without_scale = torch.softmax(qk_dot_product, dim=-1)\n",
    "attention_output_without_scale = attention_scores_without_scale @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyton-monorepo-qsUXsYbS-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
